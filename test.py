import pandas as pd
import pickle
import os
from elasticsearch import Elasticsearch
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from xgboost import XGBRegressor
from prophet import Prophet
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.arima.model import ARIMA

from datetime import datetime
from dateutil.relativedelta import relativedelta

from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

model_dir = "test_model_storage"
os.makedirs(model_dir, exist_ok=True)

# ì‹œê°í™” í•¨ìˆ˜ ì •ì˜
def plot_ts(data, color, alpha, label):
    plt.figure(figsize=(11, 5))
    plt.plot(data, color=color, alpha=alpha, label=label)
    plt.title("productQuantity of Monthly")
    plt.ylabel('productQuantity')
    plt.legend()
    plt.show()

def fetch_all_es_data(index_name, es, scroll='2m', size=1000):
    all_data = []
    page = es.search(
        index=index_name,
        scroll=scroll,
        size=size,
        body={"query": {"match_all": {}}}
    )
    sid = page['_scroll_id']
    hits = page['hits']['hits']
    all_data.extend(hits)

    while len(hits) > 0:
        page = es.scroll(scroll_id=sid, scroll=scroll)
        sid = page['_scroll_id']
        hits = page['hits']['hits']
        all_data.extend(hits)

    return [doc['_source'] for doc in all_data]
def train_timeseries_model():
    es = Elasticsearch("http://localhost:9200")
    index_name = "order_products-logs"
    data = fetch_all_es_data(index_name, es)
    df = pd.DataFrame(data)
    print(df)
    # ë‚ ì§œ ë³€í™˜ ë° í•„í„°ë§
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df["year_month"] = df["timestamp"].dt.to_period("M")

    print("ë‚ ì§œ ë³€í™˜ ë° í•„í„°ë§ ")
    print(df)

    # ì›”ë³„ íŒë§¤ëŸ‰ ì§‘ê³„
    monthly_sales = df.groupby(["productName", "year_month"])["productQuantity"].sum().reset_index()
    monthly_sales["year_month"] = monthly_sales["year_month"].astype(str)
    monthly_sales["year_month"] = pd.to_datetime(monthly_sales["year_month"])
    monthly_sales = monthly_sales.sort_values(["productName", "year_month"])

    # print("ì›”ë³„ íŒë§¤ëŸ‰", monthly_sales)
    # print(df_product)
    # df_product2 = df_product.loc[:, ['year_month', 'productQuantity']]
    # df_product2 = df_product2.set_index("year_month")
    # print(df_product2)
    # all_months = pd.date_range(
    #     start=monthly_sales["year_month"].min(),
    #     end=monthly_sales["year_month"].max(),
    #     freq="MS"
    # )
    # df_product = df_product.set_index("year_month").reindex(all_months)
    models = {}

    for product in monthly_sales["productName"].unique():
        df_product = monthly_sales[monthly_sales["productName"] == product].copy()
        # ğŸ”¹ ëˆ„ë½ëœ ì›” ì±„ìš°ê¸° (ë³´ê°„)
        all_months = pd.date_range(
            start=monthly_sales["year_month"].min(),
            end=monthly_sales["year_month"].max(),
            freq="MS"
        )
        # ë‚ ì§œë¥¼ indexë¡œ ë°”ê¿”ì„œ ì‹œê³„ì—´ ì²˜ëŸ¼ ë‹¤ë£¨ê¸° ì‰½ë„ë¡ í•¨
        df_product = df_product.set_index("year_month").reindex(all_months)
        df_product.index.name = "year_month"
        df_product["productName"] = product
        # ëˆ„ë½ ë˜ì—ˆë˜ ì›”ì˜ productQuantity ë¶€ë¶„ì˜ ë°ì´í„° ì„ í˜• ë³´ê°„
        df_product["productQuantity"] = df_product["productQuantity"].astype(float).interpolate(method="linear")
        # indexë¡œ ë³€í–ˆë˜ ë¶€ë¶„ì„ ë‹¤ì‹œ ì¼ë°˜ ì»¬ëŸ¼ìœ¼ë¡œ ë˜ëŒë¦¼
        df_product = df_product.reset_index()

        try:
            model = ARIMA(df_product["productQuantity"], order=(1, 1, 1))
            model_fit = model.fit()
            forecast = model_fit.forecast(steps=4)
            last_date = df_product["year_month"].max()

            models[product] = {
                "model": model_fit,
                "predictions": forecast.tolist(),
                "last_date": last_date
            }
        except Exception as e:
            print(f"ARIMA failed for {product}: {e}")
            continue
    df_product = monthly_sales[monthly_sales["productName"] == "ë…¸íŠ¸ë¶1"].copy()

    print(df_product)

    #plot_ts(df_product2, "blue", 0.25, 'Original')

def encode_user_info(user_info, feature_columns):
    # ì…ë ¥ëœ ìœ ì € ì •ë³´ë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ì¸ì½”ë”©
    user_df = pd.DataFrame([user_info])

    # ë²”ì£¼í˜• ìˆ˜ì¹˜í™”
    user_encoded = pd.get_dummies(user_df, columns=["region", "gender"])

    # ëˆ„ë½ëœ ë”ë¯¸ ì»¬ëŸ¼ ì¶”ê°€ (product_user_features ê¸°ì¤€ê³¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´)
    for col in feature_columns:
        if col not in user_encoded.columns:
            user_encoded[col] = 0

    # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ (ë™ì¼í•˜ê²Œ ë§ì¶°ì¤˜ì•¼ í•¨)
    user_encoded = user_encoded[feature_columns]

    return user_encoded


def train_recommend_model():
    es = Elasticsearch("http://localhost:9200")
    index_name = "order_products-logs"
    data = fetch_all_es_data(index_name, es)
    df = pd.DataFrame(data)

    # ì»¬ëŸ¼ëª… ì •ë¦¬
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df["userId"] = df["userId"].astype(str)
    df["productName"] = df["productName"].astype(str)

    df.rename(columns={
        "userAge": "age",
        "userGender": "gender",
        "userRegion": "region",
        "productName": "product"
    }, inplace=True)
    print(df)
    user_features = df[["userId", "region", "age", "gender"]].drop_duplicates()
    # 1. ë²”ì£¼í˜• ìˆ˜ì¹˜í™” (user ì •ë³´)
    user_features_encoded = pd.get_dummies(user_features.set_index("userId"), columns=["region", "gender"])

    # 2. ìœ ì € ì •ë³´ë¥¼ dfì— merge
    df_merged = df.drop(columns=["orderType"]).merge(user_features_encoded, on="userId")

    # 3. ë¬¸ìì—´ ì»¬ëŸ¼ ì œê±° (í‰ê·  ê³„ì‚°ì— í•„ìš”í•œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê¸°)
    df_numeric = df_merged.drop(columns=["region","gender","product", "userId", "sellerId", "productCategory", "timestamp"])

    # 4. ì œí’ˆë³„ ì‚¬ìš©ì íŠ¹ì„± í‰ê· 
    product_user_features = df_numeric.groupby(df_merged["product"]).mean()

    # 5. í‘œì¤€í™”
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(product_user_features)

    # 6. ìœ ì‚¬ë„ ê³„ì‚°
    similarity = cosine_similarity(scaled_features)
    similarity_df = pd.DataFrame(similarity, index=product_user_features.index, columns=product_user_features.index)

    user_info= {
        "gender" : "ë‚¨",
        "age" : 23,
        "region" : "ëŒ€ì „"
    }
    print(similarity_df.columns)
    user_vector = encode_user_info(user_info, similarity_df.columns).values.reshape(1, -1)
    print(user_vector)
    # ìœ ì‚¬ë„ ê³„ì‚° (1xN)
    product_vectors = similarity_df.values  # ê° rowëŠ” product vector
    product_names = similarity_df.index
    print(product_vectors)
    cos_scores = cosine_similarity(user_vector, product_vectors).flatten()  # ìœ ì‚¬ë„ ì ìˆ˜ (1ì°¨ì›)
    top_n_idx = cos_scores.argsort()[::-1][:5]  # ë†’ì€ ìˆœì„œ Top 5

    top_products = product_names[top_n_idx].tolist()
    print(top_products)

train_recommend_model()