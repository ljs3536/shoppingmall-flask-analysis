Index: train_recommend_product_model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pandas as pd\r\nfrom elasticsearch import Elasticsearch\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.decomposition import TruncatedSVD\r\nfrom xgboost import XGBClassifier\r\nimport pickle, os\r\nfrom sklearn.neighbors import NearestNeighbors\r\n\r\nmodel_dir = \"model_storage/recommend\"\r\nos.makedirs(model_dir, exist_ok=True)\r\n\r\ndef fetch_all_es_data(index_name, es, scroll='2m', size=1000):\r\n    all_data = []\r\n    page = es.search(index=index_name, scroll=scroll, size=size, body={\"query\": {\"match_all\": {}}})\r\n    sid = page['_scroll_id']\r\n    hits = page['hits']['hits']\r\n    all_data.extend(hits)\r\n\r\n    while hits:\r\n        page = es.scroll(scroll_id=sid, scroll=scroll)\r\n        sid = page['_scroll_id']\r\n        hits = page['hits']['hits']\r\n        all_data.extend(hits)\r\n\r\n    return [doc['_source'] for doc in all_data]\r\n\r\n\r\ndef train_recommend_model_and_save(algo_name: str):\r\n    recommendation_algos = [\"content\", \"collaborative\", \"svd\", \"xgb_classifier\", \"knn\"]\r\n    if algo_name in recommendation_algos:\r\n        return train_recommendation_model(algo_name)\r\n    else:\r\n        raise ValueError(f\"Unsupported or invalid algorithm: {algo_name}\")\r\n\r\n\r\ndef train_recommendation_model(algo_name: str):\r\n    es = Elasticsearch(\"http://localhost:9200\")\r\n    index_name = \"order_products-logs\"\r\n    data = fetch_all_es_data(index_name, es)\r\n    df = pd.DataFrame(data)\r\n\r\n    # ì»¬ëŸ¼ëª… ì •ë¦¬\r\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\r\n    df[\"userId\"] = df[\"userId\"].astype(str)\r\n    df[\"productName\"] = df[\"productName\"].astype(str)\r\n\r\n    # í•„ë“œëª… ë§¤ì¹­\r\n    df.rename(columns={\r\n        \"userAge\": \"age\",\r\n        \"userGender\": \"gender\",\r\n        \"userRegion\": \"region\",\r\n        \"productName\": \"product\"\r\n    }, inplace=True)\r\n\r\n    user_features = df[[\"userId\", \"region\", \"age\", \"gender\"]].drop_duplicates()\r\n\r\n    model_path = os.path.join(model_dir, f\"model_{algo_name}.pkl\")\r\n\r\n    if algo_name == \"content\":\r\n\r\n        # 1. ë²”ì£¼í˜• ìˆ˜ì¹˜í™” (user ì •ë³´)\r\n        user_features_encoded = pd.get_dummies(user_features.set_index(\"userId\"), columns=[\"region\", \"gender\"])\r\n\r\n        # 2. ìœ ì € ì •ë³´ë¥¼ dfì— merge\r\n        df_merged = df.drop(columns=[\"orderType\"]).merge(user_features_encoded, on=\"userId\")\r\n\r\n        # 3. ë¬¸ìì—´ ì»¬ëŸ¼ ì œê±° (í‰ê·  ê³„ì‚°ì— í•„ìš”í•œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê¸°)\r\n        df_numeric = df_merged.drop(columns=[\"region\",\"gender\",\"product\", \"userId\", \"sellerId\", \"productCategory\", \"timestamp\"])\r\n\r\n        # 4. ì œí’ˆë³„ ì‚¬ìš©ì íŠ¹ì„± í‰ê· \r\n        product_user_features = df_numeric.groupby(df_merged[\"product\"]).mean()\r\n\r\n        # 5. í‘œì¤€í™”\r\n        scaler = StandardScaler()\r\n        scaled_features = scaler.fit_transform(product_user_features)\r\n\r\n        # 6. ìœ ì‚¬ë„ ê³„ì‚°\r\n        similarity = cosine_similarity(scaled_features)\r\n        similarity_df = pd.DataFrame(similarity, index=product_user_features.index, columns=product_user_features.index)\r\n\r\n        with open(model_path, \"wb\") as f:\r\n            pickle.dump(similarity_df, f)\r\n\r\n        return {\"message\": \"Content-based filtering model trained and saved.\"}\r\n\r\n    elif algo_name == \"collaborative\":\r\n        user_item_matrix = df.groupby([\"userId\", \"product\"]).size().unstack(fill_value=0)\r\n        similarity = cosine_similarity(user_item_matrix)\r\n        similarity_df = pd.DataFrame(similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\r\n\r\n        with open(model_path, \"wb\") as f:\r\n            pickle.dump(similarity_df, f)\r\n\r\n        return {\"message\": \"Collaborative filtering model trained and saved.\"}\r\n\r\n    elif algo_name == \"svd\":\r\n        user_item_matrix = df.groupby([\"userId\", \"product\"]).size().unstack(fill_value=0)\r\n        svd = TruncatedSVD(n_components=10)\r\n        svd_matrix = svd.fit_transform(user_item_matrix)\r\n\r\n        model_data = {\r\n            \"svd\": svd,\r\n            \"user_index\": user_item_matrix.index.tolist(),\r\n            \"item_columns\": user_item_matrix.columns.tolist()\r\n        }\r\n\r\n        with open(model_path, \"wb\") as f:\r\n            pickle.dump(model_data, f)\r\n\r\n        return {\"message\": \"SVD recommendation model trained and saved.\"}\r\n\r\n    elif algo_name == \"xgb_classifier\":\r\n        # êµ¬ë§¤ëœ ì¡°í•© (positive sample)\r\n        df[\"label\"] = 1\r\n        positive_df = df[[\"userId\", \"product\", \"age\", \"gender\", \"region\", \"label\"]]\r\n\r\n        # ì‚¬ìš©ì ë° ìƒí’ˆ ëª©ë¡\r\n        users = df[\"userId\"].unique()\r\n        products = df[\"product\"].unique()\r\n\r\n        # ìŒì„± ìƒ˜í”Œ ìƒì„± (ëœë¤ ì‚¬ìš©ì-ìƒí’ˆ ì¡°í•© ì¤‘ ì‹¤ì œ êµ¬ë§¤í•œ ê²ƒ ì œì™¸)\r\n        import random\r\n\r\n        neg_samples = []\r\n        existing_pairs = set(zip(df[\"userId\"], df[\"product\"]))\r\n\r\n        while len(neg_samples) < len(positive_df) * 0.5:\r\n            user = random.choice(users)\r\n            product = random.choice(products)\r\n            if (user, product) not in existing_pairs:\r\n                user_info = user_features.loc[user]\r\n                neg_samples.append({\r\n                    \"userId\": user,\r\n                    \"product\": product,\r\n                    \"age\": user_info[\"age\"],\r\n                    \"gender\": user_info[\"gender\"],\r\n                    \"region\": user_info[\"region\"],\r\n                    \"label\": 0\r\n                })\r\n\r\n        negative_df = pd.DataFrame(neg_samples)\r\n        train_df = pd.concat([positive_df, negative_df], ignore_index=True)\r\n\r\n        product_encoder = {k: v for v, k in enumerate(df[\"product\"].astype(\"category\").cat.categories)}\r\n        region_encoder = {k: v for v, k in enumerate(df[\"region\"].astype(\"category\").cat.categories)}\r\n\r\n        X = pd.DataFrame({\r\n            \"age\": train_df[\"age\"],\r\n            \"gender\": train_df[\"gender\"].map({\"ë‚¨\": 0, \"ì—¬\": 1}),\r\n            \"region\": train_df[\"region\"].map(region_encoder),\r\n            \"product\": train_df[\"product\"].map(product_encoder)\r\n        })\r\n        y = train_df[\"label\"]\r\n\r\n        model = XGBClassifier(n_estimators=50, max_depth=4, learning_rate=0.2,\r\n                              use_label_encoder=False, eval_metric='logloss')\r\n        model.fit(X, y)\r\n\r\n        model_data = {\r\n            \"model\": model,\r\n            \"product_encoder\": product_encoder,\r\n            \"region_encoder\": region_encoder,\r\n        }\r\n\r\n        with open(model_path, \"wb\") as f:\r\n            pickle.dump(model_data, f)\r\n\r\n        return {\"message\": \"XGBoost classifier recommendation model trained and saved.\"}\r\n\r\n\r\n    elif algo_name == \"knn\":\r\n        user_item_matrix = df.groupby([\"userId\", \"product\"]).size().unstack(fill_value=0)\r\n        knn = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\r\n        knn.fit(user_item_matrix)\r\n\r\n        model_data = {\r\n            \"knn\": knn,\r\n            \"user_index\": user_item_matrix.index.tolist(),\r\n            \"product_columns\": user_item_matrix.columns.tolist(),\r\n            \"user_item_matrix\": user_item_matrix.values  # ì‚¬ìš©ì ë²¡í„° ì €ì¥\r\n        }\r\n\r\n        with open(model_path, \"wb\") as f:\r\n            pickle.dump(model_data, f)\r\n\r\n        return {\"message\": \"k-NN recommendation model trained and saved.\"}\r\n\r\n    else:\r\n        raise ValueError(f\"Unsupported recommendation model: {algo_name}\")\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/train_recommend_product_model.py b/train_recommend_product_model.py
--- a/train_recommend_product_model.py	(revision 94921fb68d3aad3ebf0cedff4b49038b84810eb7)
+++ b/train_recommend_product_model.py	(date 1744361264156)
@@ -79,8 +79,12 @@
         similarity = cosine_similarity(scaled_features)
         similarity_df = pd.DataFrame(similarity, index=product_user_features.index, columns=product_user_features.index)
 
+        model = {
+            "similarity_df": similarity_df,
+            "feature_columns": list(product_user_features.columns)  # ì´ê±¸ ì¶”ê°€ ì €ì¥
+        }
         with open(model_path, "wb") as f:
-            pickle.dump(similarity_df, f)
+            pickle.dump(model, f)
 
         return {"message": "Content-based filtering model trained and saved."}
 
Index: train_order_product_model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pandas as pd\r\nimport pickle\r\nimport os\r\nfrom elasticsearch import Elasticsearch\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\nfrom xgboost import XGBRegressor\r\nfrom prophet import Prophet\r\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\r\nfrom statsmodels.tsa.arima.model import ARIMA\r\n\r\nfrom datetime import datetime\r\nfrom dateutil.relativedelta import relativedelta\r\n\r\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\r\nfrom sklearn.tree import DecisionTreeRegressor\r\nfrom xgboost import XGBRegressor\r\n\r\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.pipeline import Pipeline\r\nfrom scipy.sparse import csr_matrix\r\nfrom implicit.als import AlternatingLeastSquares\r\n\r\nmodel_dir = \"model_storage/predict\"\r\nos.makedirs(model_dir, exist_ok=True)\r\n\r\ndef fetch_all_es_data(index_name, es, scroll='2m', size=1000):\r\n    all_data = []\r\n    page = es.search(\r\n        index=index_name,\r\n        scroll=scroll,\r\n        size=size,\r\n        body={\"query\": {\"match_all\": {}}}\r\n    )\r\n    sid = page['_scroll_id']\r\n    hits = page['hits']['hits']\r\n    all_data.extend(hits)\r\n\r\n    while len(hits) > 0:\r\n        page = es.scroll(scroll_id=sid, scroll=scroll)\r\n        sid = page['_scroll_id']\r\n        hits = page['hits']['hits']\r\n        all_data.extend(hits)\r\n\r\n    return [doc['_source'] for doc in all_data]\r\n\r\ndef train_predict_model_and_save(algo_name: str):\r\n    # if algo_name in [\"linear\", \"logistic\", \"tree\", \"xgb\"]:\r\n    #     return train_standard_model(algo_name)\r\n    # el\r\n    timeseries_algos = [\"xgb_timeseries\", \"prophet\", \"arima\", \"sarimax\"]\r\n    if algo_name in timeseries_algos:\r\n        return train_timeseries_model(algo_name)\r\n    else:\r\n        raise ValueError(f\"Unsupported or invalid algorithm: {algo_name}\")\r\n\r\ndef train_timeseries_model(algo_name: str):\r\n    es = Elasticsearch(\"http://localhost:9200\")\r\n    index_name = \"order_products-logs\"\r\n    data = fetch_all_es_data(index_name, es)\r\n    df = pd.DataFrame(data)\r\n\r\n    # ë‚ ì§œ ë³€í™˜ ë° í•„í„°ë§\r\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\r\n    df[\"year_month\"] = df[\"timestamp\"].dt.to_period(\"M\")\r\n\r\n    # ì›”ë³„ íŒë§¤ëŸ‰ ì§‘ê³„\r\n    monthly_sales = df.groupby([\"productName\", \"year_month\"])[\"productQuantity\"].sum().reset_index()\r\n    monthly_sales[\"year_month\"] = monthly_sales[\"year_month\"].astype(str)\r\n    monthly_sales[\"year_month\"] = pd.to_datetime(monthly_sales[\"year_month\"])\r\n    monthly_sales = monthly_sales.sort_values([\"productName\", \"year_month\"])\r\n\r\n    models = {}\r\n\r\n    for product in monthly_sales[\"productName\"].unique():\r\n        df_product = monthly_sales[monthly_sales[\"productName\"] == product].copy()\r\n        # \uD83D\uDD39 ëˆ„ë½ëœ ì›” ì±„ìš°ê¸° (ë³´ê°„)\r\n        all_months = pd.date_range(\r\n            start=monthly_sales[\"year_month\"].min(),\r\n            end=monthly_sales[\"year_month\"].max(),\r\n            freq=\"MS\"\r\n        )\r\n        # ë‚ ì§œë¥¼ indexë¡œ ë°”ê¿”ì„œ ì‹œê³„ì—´ ì²˜ëŸ¼ ë‹¤ë£¨ê¸° ì‰½ë„ë¡ í•¨\r\n        df_product = df_product.set_index(\"year_month\").reindex(all_months)\r\n        df_product.index.name = \"year_month\"\r\n        df_product[\"productName\"] = product\r\n        # ëˆ„ë½ ë˜ì—ˆë˜ ì›”ì˜ productQuantity ë¶€ë¶„ì˜ ë°ì´í„° ì„ í˜• ë³´ê°„\r\n        df_product[\"productQuantity\"] = df_product[\"productQuantity\"].astype(float).interpolate(method=\"linear\")\r\n        # indexë¡œ ë³€í–ˆë˜ ë¶€ë¶„ì„ ë‹¤ì‹œ ì¼ë°˜ ì»¬ëŸ¼ìœ¼ë¡œ ë˜ëŒë¦¼\r\n        df_product = df_product.reset_index()\r\n\r\n        if len(df_product) < 1:\r\n            continue  # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ\r\n\r\n        if algo_name == \"xgb_timeseries\":\r\n            # lag feature ìƒì„±\r\n            n_lags = 3\r\n            for lag in range(1, n_lags + 1):\r\n                df_product[f\"lag_{lag}\"] = df_product[\"productQuantity\"].shift(lag)\r\n            df_product[\"target\"] = df_product[\"productQuantity\"].shift(-1)\r\n            df_product.dropna(inplace=True)\r\n\r\n            X = df_product[[\"lag_1\", \"lag_2\", \"lag_3\"]]\r\n            y = df_product[\"target\"]\r\n            model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, objective='reg:squarederror')\r\n            model.fit(X, y)\r\n\r\n            last_date = df_product[\"year_month\"].max()\r\n\r\n            # í–¥í›„ 4ê°œì›” ì˜ˆì¸¡\r\n            future_preds = []\r\n            last_vals = list(df_product.iloc[-3:][\"productQuantity\"])\r\n            for _ in range(4):\r\n                x_input = pd.DataFrame([last_vals[-3:]], columns=[\"lag_1\", \"lag_2\", \"lag_3\"])\r\n                pred = model.predict(x_input)[0]\r\n                future_preds.append(pred)\r\n                last_vals.append(pred)\r\n\r\n            models[product] = {\r\n                \"model\": model,\r\n                \"last_vals\": df_product.iloc[-3:][\"productQuantity\"].tolist(),\r\n                \"predictions\": future_preds,\r\n                \"last_date\" : last_date\r\n            }\r\n\r\n        elif algo_name == \"prophet\":\r\n            df_prophet = df_product.rename(columns={\"year_month\": \"ds\", \"productQuantity\": \"y\"})\r\n            model = Prophet()\r\n            model.fit(df_prophet)\r\n\r\n            last_date = df_prophet[\"ds\"].max()\r\n            future = pd.date_range(start=last_date + relativedelta(months=1), periods=4, freq=\"MS\")\r\n            future_df = pd.DataFrame({\"ds\": future})\r\n\r\n            forecast = model.predict(future_df)\r\n            preds = forecast[\"yhat\"].tolist()\r\n\r\n            models[product] = {\r\n                \"model\": model,\r\n                \"predictions\": preds,\r\n                \"last_date\" : last_date\r\n            }\r\n        elif algo_name == \"arima\":\r\n            try:\r\n                model = ARIMA(df_product[\"productQuantity\"], order=(1, 1, 1))\r\n                model_fit = model.fit()\r\n                forecast = model_fit.forecast(steps=4)\r\n                last_date = df_product[\"year_month\"].max()\r\n\r\n                models[product] = {\r\n                    \"model\": model_fit,\r\n                    \"predictions\": forecast.tolist(),\r\n                    \"last_date\" : last_date\r\n                }\r\n            except Exception as e:\r\n                print(f\"ARIMA failed for {product}: {e}\")\r\n                continue\r\n\r\n        elif algo_name == \"sarimax\":\r\n            try:\r\n                model = SARIMAX(df_product[\"productQuantity\"], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\r\n                model_fit = model.fit(disp=False)\r\n                forecast = model_fit.forecast(steps=4)\r\n                last_date = df_product[\"year_month\"].max()\r\n                models[product] = {\r\n                    \"model\": model_fit,\r\n                    \"predictions\": forecast.tolist(),\r\n                    \"last_date\": last_date\r\n                }\r\n            except Exception as e:\r\n                print(f\"SARIMAX failed for {product}: {e}\")\r\n                continue\r\n\r\n    # \uD83D\uDD39 ì €ì¥\r\n    model_path = os.path.join(model_dir, f\"model_{algo_name}.pkl\")\r\n    with open(model_path, \"wb\") as f:\r\n        pickle.dump(models, f)\r\n\r\n    print(models.keys())\r\n    return {\"message\": f\"{algo_name} time series model trained and saved.\"}\r\n\r\n# def get_model_by_name(algo_name: str):\r\n#     if algo_name == \"linear\":\r\n#         return LinearRegression()\r\n#     elif algo_name == \"logistic\":\r\n#         return LogisticRegression()\r\n#     elif algo_name == \"tree\":\r\n#         return DecisionTreeRegressor()\r\n#     elif algo_name == \"xgb\":\r\n#         return XGBRegressor()\r\n#     else:\r\n#         raise ValueError(f\"Unsupported algorithm: {algo_name}\")\r\n\r\n# def train_standard_model(algo_name: str):\r\n#     es = Elasticsearch(\"http://localhost:9200\")\r\n#     index_name = \"order_products-logs\"\r\n#\r\n#     res = es.search(index=index_name, body={\"size\": 10000, \"query\": {\"match_all\": {}}})\r\n#     data = [hit['_source'] for hit in res['hits']['hits']]\r\n#     df = pd.DataFrame(data)\r\n#\r\n#     # \uD83D\uDD39 ë‚ ì§œ ì „ì²˜ë¦¬\r\n#     df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\r\n#     df[\"month\"] = df[\"timestamp\"].dt.month\r\n#     df[\"year_month\"] = df[\"timestamp\"].dt.to_period(\"M\")\r\n#\r\n#     # \uD83D\uDD39 ì„±ë³„ ì „ì²˜ë¦¬ (ë‚¨: 0, ì—¬: 1)\r\n#     df[\"userGender\"] = df[\"userGender\"].map({\"ë‚¨\": 0, \"ì—¬\": 1})\r\n#\r\n#     # \uD83D\uDD39 ì „ì›” íŒë§¤ëŸ‰ ê³„ì‚° â†’ ì¦ê°€ìœ¨ (product ê¸°ì¤€)\r\n#     monthly_sales = df.groupby([\"productName\", \"year_month\"])[\"productQuantity\"].sum().reset_index()\r\n#     monthly_sales[\"prev_month_qty\"] = monthly_sales.groupby(\"productName\")[\"productQuantity\"].shift(1)\r\n#     monthly_sales[\"increase_rate\"] = (\r\n#         (monthly_sales[\"productQuantity\"] - monthly_sales[\"prev_month_qty\"]) /\r\n#         monthly_sales[\"prev_month_qty\"]\r\n#     ).fillna(0)\r\n#\r\n#     # \uD83D\uDD39 ì›ë³¸ dfì™€ ë³‘í•©\r\n#     df[\"year_month\"] = df[\"timestamp\"].dt.to_period(\"M\")\r\n#     df = pd.merge(df, monthly_sales[[\"productName\", \"year_month\", \"increase_rate\"]],\r\n#                   on=[\"productName\", \"year_month\"], how=\"left\")\r\n#\r\n#     df[\"increase_rate\"] = df[\"increase_rate\"].fillna(0)\r\n#\r\n#     # \uD83D\uDD39 í•„ìš”í•œ ì—´ë§Œ ì¶”ì¶œ\r\n#     features = [\"userAge\", \"productPrice\", \"userGender\", \"userRegion\", \"month\", \"increase_rate\"]\r\n#     df_model = df[features + [\"productName\", \"productQuantity\"]]\r\n#\r\n#     # \uD83D\uDD39 í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„: ì œí’ˆë³„ í‰ê· ê°’ ì‚¬ìš©\r\n#     df_grouped = df_model.groupby(\"productName\").agg({\r\n#         \"userAge\": \"mean\",\r\n#         \"productPrice\": \"mean\",\r\n#         \"userGender\": \"mean\",\r\n#         \"month\": \"mean\",\r\n#         \"increase_rate\": \"mean\",\r\n#         \"userRegion\": lambda x: x.mode()[0],  # ìµœë¹ˆê°’ ì‚¬ìš©\r\n#         \"productQuantity\": \"sum\"\r\n#     }).reset_index()\r\n#\r\n#     # \uD83D\uDD39 Feature / Target êµ¬ë¶„\r\n#     X = df_grouped[[\"userAge\", \"productPrice\", \"userGender\", \"userRegion\", \"month\", \"increase_rate\"]]\r\n#     y = df_grouped[\"productQuantity\"]\r\n#\r\n#     # \uD83D\uDD39 ë²”ì£¼í˜• ì¸ì½”ë”© (userRegion)\r\n#     categorical_features = [\"userRegion\"]\r\n#     numeric_features = [\"userAge\", \"productPrice\", \"userGender\", \"month\", \"increase_rate\"]\r\n#\r\n#     preprocessor = ColumnTransformer(transformers=[\r\n#         (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\r\n#     ], remainder=\"passthrough\")\r\n#\r\n#     model = get_model_by_name(algo_name)\r\n#\r\n#     # \uD83D\uDD39 íŒŒì´í”„ë¼ì¸ êµ¬ì„±\r\n#     pipeline = Pipeline(steps=[\r\n#         (\"preprocessor\", preprocessor),\r\n#         (\"model\", model)\r\n#     ])\r\n#\r\n#     pipeline.fit(X, y)\r\n#\r\n#     # \uD83D\uDD39 ì €ì¥\r\n#     model_path = os.path.join(model_dir, f\"model_{algo_name}.pkl\")\r\n#     with open(model_path, \"wb\") as f:\r\n#         pickle.dump((pipeline, df_grouped), f)\r\n#\r\n#     return {\"message\": f\"{algo_name} model trained and saved.\"}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/train_order_product_model.py b/train_order_product_model.py
--- a/train_order_product_model.py	(revision 94921fb68d3aad3ebf0cedff4b49038b84810eb7)
+++ b/train_order_product_model.py	(date 1744360115164)
@@ -74,7 +74,7 @@
 
     for product in monthly_sales["productName"].unique():
         df_product = monthly_sales[monthly_sales["productName"] == product].copy()
-        # ğŸ”¹ ëˆ„ë½ëœ ì›” ì±„ìš°ê¸° (ë³´ê°„)
+        # ëˆ„ë½ëœ ì›” ì±„ìš°ê¸° (ë³´ê°„)
         all_months = pd.date_range(
             start=monthly_sales["year_month"].min(),
             end=monthly_sales["year_month"].max(),
Index: test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pandas as pd\r\nimport pickle\r\nimport os\r\nfrom elasticsearch import Elasticsearch\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\nfrom xgboost import XGBRegressor\r\nfrom prophet import Prophet\r\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\r\nfrom statsmodels.tsa.arima.model import ARIMA\r\n\r\nfrom datetime import datetime\r\nfrom dateutil.relativedelta import relativedelta\r\n\r\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\r\nfrom sklearn.tree import DecisionTreeRegressor\r\nfrom xgboost import XGBRegressor\r\n\r\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.pipeline import Pipeline\r\n\r\nmodel_dir = \"test_model_storage\"\r\nos.makedirs(model_dir, exist_ok=True)\r\n\r\n# ì‹œê°í™” í•¨ìˆ˜ ì •ì˜\r\ndef plot_ts(data, color, alpha, label):\r\n    plt.figure(figsize=(11, 5))\r\n    plt.plot(data, color=color, alpha=alpha, label=label)\r\n    plt.title(\"productQuantity of Monthly\")\r\n    plt.ylabel('productQuantity')\r\n    plt.legend()\r\n    plt.show()\r\n\r\ndef fetch_all_es_data(index_name, es, scroll='2m', size=1000):\r\n    all_data = []\r\n    page = es.search(\r\n        index=index_name,\r\n        scroll=scroll,\r\n        size=size,\r\n        body={\"query\": {\"match_all\": {}}}\r\n    )\r\n    sid = page['_scroll_id']\r\n    hits = page['hits']['hits']\r\n    all_data.extend(hits)\r\n\r\n    while len(hits) > 0:\r\n        page = es.scroll(scroll_id=sid, scroll=scroll)\r\n        sid = page['_scroll_id']\r\n        hits = page['hits']['hits']\r\n        all_data.extend(hits)\r\n\r\n    return [doc['_source'] for doc in all_data]\r\ndef train_timeseries_model():\r\n    es = Elasticsearch(\"http://localhost:9200\")\r\n    index_name = \"order_products-logs\"\r\n    data = fetch_all_es_data(index_name, es)\r\n    df = pd.DataFrame(data)\r\n    print(df)\r\n    # ë‚ ì§œ ë³€í™˜ ë° í•„í„°ë§\r\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\r\n    df[\"year_month\"] = df[\"timestamp\"].dt.to_period(\"M\")\r\n\r\n    print(\"ë‚ ì§œ ë³€í™˜ ë° í•„í„°ë§ \")\r\n    print(df)\r\n\r\n    # ì›”ë³„ íŒë§¤ëŸ‰ ì§‘ê³„\r\n    monthly_sales = df.groupby([\"productName\", \"year_month\"])[\"productQuantity\"].sum().reset_index()\r\n    monthly_sales[\"year_month\"] = monthly_sales[\"year_month\"].astype(str)\r\n    monthly_sales[\"year_month\"] = pd.to_datetime(monthly_sales[\"year_month\"])\r\n    monthly_sales = monthly_sales.sort_values([\"productName\", \"year_month\"])\r\n\r\n    # print(\"ì›”ë³„ íŒë§¤ëŸ‰\", monthly_sales)\r\n    # print(df_product)\r\n    # df_product2 = df_product.loc[:, ['year_month', 'productQuantity']]\r\n    # df_product2 = df_product2.set_index(\"year_month\")\r\n    # print(df_product2)\r\n    # all_months = pd.date_range(\r\n    #     start=monthly_sales[\"year_month\"].min(),\r\n    #     end=monthly_sales[\"year_month\"].max(),\r\n    #     freq=\"MS\"\r\n    # )\r\n    # df_product = df_product.set_index(\"year_month\").reindex(all_months)\r\n    models = {}\r\n\r\n    for product in monthly_sales[\"productName\"].unique():\r\n        df_product = monthly_sales[monthly_sales[\"productName\"] == product].copy()\r\n        # \uD83D\uDD39 ëˆ„ë½ëœ ì›” ì±„ìš°ê¸° (ë³´ê°„)\r\n        all_months = pd.date_range(\r\n            start=monthly_sales[\"year_month\"].min(),\r\n            end=monthly_sales[\"year_month\"].max(),\r\n            freq=\"MS\"\r\n        )\r\n        # ë‚ ì§œë¥¼ indexë¡œ ë°”ê¿”ì„œ ì‹œê³„ì—´ ì²˜ëŸ¼ ë‹¤ë£¨ê¸° ì‰½ë„ë¡ í•¨\r\n        df_product = df_product.set_index(\"year_month\").reindex(all_months)\r\n        df_product.index.name = \"year_month\"\r\n        df_product[\"productName\"] = product\r\n        # ëˆ„ë½ ë˜ì—ˆë˜ ì›”ì˜ productQuantity ë¶€ë¶„ì˜ ë°ì´í„° ì„ í˜• ë³´ê°„\r\n        df_product[\"productQuantity\"] = df_product[\"productQuantity\"].astype(float).interpolate(method=\"linear\")\r\n        # indexë¡œ ë³€í–ˆë˜ ë¶€ë¶„ì„ ë‹¤ì‹œ ì¼ë°˜ ì»¬ëŸ¼ìœ¼ë¡œ ë˜ëŒë¦¼\r\n        df_product = df_product.reset_index()\r\n\r\n        try:\r\n            model = ARIMA(df_product[\"productQuantity\"], order=(1, 1, 1))\r\n            model_fit = model.fit()\r\n            forecast = model_fit.forecast(steps=4)\r\n            last_date = df_product[\"year_month\"].max()\r\n\r\n            models[product] = {\r\n                \"model\": model_fit,\r\n                \"predictions\": forecast.tolist(),\r\n                \"last_date\": last_date\r\n            }\r\n        except Exception as e:\r\n            print(f\"ARIMA failed for {product}: {e}\")\r\n            continue\r\n    df_product = monthly_sales[monthly_sales[\"productName\"] == \"ë…¸íŠ¸ë¶1\"].copy()\r\n\r\n    print(df_product)\r\n\r\n    #plot_ts(df_product2, \"blue\", 0.25, 'Original')\r\n\r\ndef encode_user_info(user_info, feature_columns):\r\n    # ì…ë ¥ëœ ìœ ì € ì •ë³´ë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ì¸ì½”ë”©\r\n    user_df = pd.DataFrame([user_info])\r\n\r\n    # ë²”ì£¼í˜• ìˆ˜ì¹˜í™”\r\n    user_encoded = pd.get_dummies(user_df, columns=[\"region\", \"gender\"])\r\n\r\n    # ëˆ„ë½ëœ ë”ë¯¸ ì»¬ëŸ¼ ì¶”ê°€ (product_user_features ê¸°ì¤€ê³¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´)\r\n    for col in feature_columns:\r\n        if col not in user_encoded.columns:\r\n            user_encoded[col] = 0\r\n\r\n    # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ (ë™ì¼í•˜ê²Œ ë§ì¶°ì¤˜ì•¼ í•¨)\r\n    user_encoded = user_encoded[feature_columns]\r\n\r\n    return user_encoded\r\n\r\n\r\ndef train_recommend_model():\r\n    es = Elasticsearch(\"http://localhost:9200\")\r\n    index_name = \"order_products-logs\"\r\n    data = fetch_all_es_data(index_name, es)\r\n    df = pd.DataFrame(data)\r\n\r\n    # ì»¬ëŸ¼ëª… ì •ë¦¬\r\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\r\n    df[\"userId\"] = df[\"userId\"].astype(str)\r\n    df[\"productName\"] = df[\"productName\"].astype(str)\r\n\r\n    df.rename(columns={\r\n        \"userAge\": \"age\",\r\n        \"userGender\": \"gender\",\r\n        \"userRegion\": \"region\",\r\n        \"productName\": \"product\"\r\n    }, inplace=True)\r\n    print(df)\r\n    user_features = df[[\"userId\", \"region\", \"age\", \"gender\"]].drop_duplicates()\r\n    # 1. ë²”ì£¼í˜• ìˆ˜ì¹˜í™” (user ì •ë³´)\r\n    user_features_encoded = pd.get_dummies(user_features.set_index(\"userId\"), columns=[\"region\", \"gender\"])\r\n\r\n    # 2. ìœ ì € ì •ë³´ë¥¼ dfì— merge\r\n    df_merged = df.drop(columns=[\"orderType\"]).merge(user_features_encoded, on=\"userId\")\r\n\r\n    # 3. ë¬¸ìì—´ ì»¬ëŸ¼ ì œê±° (í‰ê·  ê³„ì‚°ì— í•„ìš”í•œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê¸°)\r\n    df_numeric = df_merged.drop(columns=[\"region\",\"gender\",\"product\", \"userId\", \"sellerId\", \"productCategory\", \"timestamp\"])\r\n\r\n    # 4. ì œí’ˆë³„ ì‚¬ìš©ì íŠ¹ì„± í‰ê· \r\n    product_user_features = df_numeric.groupby(df_merged[\"product\"]).mean()\r\n\r\n    # 5. í‘œì¤€í™”\r\n    scaler = StandardScaler()\r\n    scaled_features = scaler.fit_transform(product_user_features)\r\n\r\n    # 6. ìœ ì‚¬ë„ ê³„ì‚°\r\n    similarity = cosine_similarity(scaled_features)\r\n    similarity_df = pd.DataFrame(similarity, index=product_user_features.index, columns=product_user_features.index)\r\n\r\n    user_info= {\r\n        \"gender\" : \"ë‚¨\",\r\n        \"age\" : 23,\r\n        \"region\" : \"ëŒ€ì „\"\r\n    }\r\n    print(similarity_df.columns)\r\n    user_vector = encode_user_info(user_info, similarity_df.columns).values.reshape(1, -1)\r\n    print(user_vector)\r\n    # ìœ ì‚¬ë„ ê³„ì‚° (1xN)\r\n    product_vectors = similarity_df.values  # ê° rowëŠ” product vector\r\n    product_names = similarity_df.index\r\n    print(product_vectors)\r\n    cos_scores = cosine_similarity(user_vector, product_vectors).flatten()  # ìœ ì‚¬ë„ ì ìˆ˜ (1ì°¨ì›)\r\n    top_n_idx = cos_scores.argsort()[::-1][:5]  # ë†’ì€ ìˆœì„œ Top 5\r\n\r\n    top_products = product_names[top_n_idx].tolist()\r\n    print(top_products)\r\n\r\ntrain_recommend_model()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test.py b/test.py
--- a/test.py	(revision 94921fb68d3aad3ebf0cedff4b49038b84810eb7)
+++ b/test.py	(date 1744361994792)
@@ -85,7 +85,7 @@
 
     for product in monthly_sales["productName"].unique():
         df_product = monthly_sales[monthly_sales["productName"] == product].copy()
-        # ğŸ”¹ ëˆ„ë½ëœ ì›” ì±„ìš°ê¸° (ë³´ê°„)
+        # ëˆ„ë½ëœ ì›” ì±„ìš°ê¸° (ë³´ê°„)
         all_months = pd.date_range(
             start=monthly_sales["year_month"].min(),
             end=monthly_sales["year_month"].max(),
@@ -128,6 +128,7 @@
     user_encoded = pd.get_dummies(user_df, columns=["region", "gender"])
 
     # ëˆ„ë½ëœ ë”ë¯¸ ì»¬ëŸ¼ ì¶”ê°€ (product_user_features ê¸°ì¤€ê³¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´)
+
     for col in feature_columns:
         if col not in user_encoded.columns:
             user_encoded[col] = 0
@@ -156,7 +157,7 @@
         "productName": "product"
     }, inplace=True)
     print(df)
-    user_features = df[["userId", "region", "age", "gender"]].drop_duplicates()
+    user_features = df[["userId", "region", "gender"]].drop_duplicates()
     # 1. ë²”ì£¼í˜• ìˆ˜ì¹˜í™” (user ì •ë³´)
     user_features_encoded = pd.get_dummies(user_features.set_index("userId"), columns=["region", "gender"])
 
@@ -164,8 +165,8 @@
     df_merged = df.drop(columns=["orderType"]).merge(user_features_encoded, on="userId")
 
     # 3. ë¬¸ìì—´ ì»¬ëŸ¼ ì œê±° (í‰ê·  ê³„ì‚°ì— í•„ìš”í•œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê¸°)
-    df_numeric = df_merged.drop(columns=["region","gender","product", "userId", "sellerId", "productCategory", "timestamp"])
-
+    df_numeric = df_merged.drop(columns=["productPrice","productQuantity","region","gender","product", "userId", "sellerId", "productCategory", "timestamp"])
+    print(df_numeric)
     # 4. ì œí’ˆë³„ ì‚¬ìš©ì íŠ¹ì„± í‰ê· 
     product_user_features = df_numeric.groupby(df_merged["product"]).mean()
 
@@ -182,12 +183,12 @@
         "age" : 23,
         "region" : "ëŒ€ì „"
     }
-    print(similarity_df.columns)
-    user_vector = encode_user_info(user_info, similarity_df.columns).values.reshape(1, -1)
+    print(similarity_df.values)
+    user_vector = encode_user_info(user_info, product_user_features.columns).values.reshape(1, -1)
     print(user_vector)
     # ìœ ì‚¬ë„ ê³„ì‚° (1xN)
-    product_vectors = similarity_df.values  # ê° rowëŠ” product vector
-    product_names = similarity_df.index
+    product_vectors = scaled_features  # ê° rowëŠ” product vector
+    product_names = product_user_features.index
     print(product_vectors)
     cos_scores = cosine_similarity(user_vector, product_vectors).flatten()  # ìœ ì‚¬ë„ ì ìˆ˜ (1ì°¨ì›)
     top_n_idx = cos_scores.argsort()[::-1][:5]  # ë†’ì€ ìˆœì„œ Top 5
Index: app.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from flask import Flask, jsonify, request\r\nfrom elasticsearch import Elasticsearch\r\nfrom generate_cart_logs import generate_cart_log\r\nfrom generate_order_logs import generate_order_log\r\nfrom search_handler import get_yearly_sales, get_age_group_favorites, get_region_favorites, get_monthly_category_trend, get_gender_favorites\r\nfrom train_order_product_model import train_predict_model_and_save\r\nfrom predict_order_product_model import predict_quantity_pipeline\r\nfrom train_recommend_product_model import train_recommend_model_and_save\r\nfrom predict_recommend_product_model import predict_recommendation_pipeline\r\nfrom search_Recommend import get_trendingProducts, get_addedCartProducts, get_moreSellingProducts, get_popularProducts_category, get_highRatedProducts\r\napp = Flask(__name__)\r\n\r\n# Elasticsearch ì—°ê²° (Docker ì»¨í…Œì´ë„ˆì—ì„œ ì‹¤í–‰ ì¤‘ì¼ ê²½ìš°)\r\nes = Elasticsearch(\"http://localhost:9200\")\r\n\r\n@app.route(\"/search\", methods=[\"GET\"])\r\ndef search_logs():\r\n    keyword = request.args.get(\"keyword\", \"\")\r\n    index_name = \"access-log\"  # ë¡œê·¸ê°€ ì €ì¥ëœ ì¸ë±ìŠ¤ ì´ë¦„\r\n\r\n    query = {\r\n        \"query\": {\r\n            \"match\": {\r\n                \"userId\": keyword\r\n            }\r\n        }\r\n    }\r\n\r\n    res = es.search(index=index_name, body=query)\r\n    return jsonify(res[\"hits\"][\"hits\"])\r\n\r\n# /generate/cart ì—”ë“œí¬ì¸íŠ¸\r\n@app.route('/generate/cart', methods=['GET'])\r\ndef generate_cart_logs():\r\n    generate_cart_log()\r\n    return jsonify({\"message\": \"Cart logs generated!\"})\r\n\r\n# /generate/order ì—”ë“œí¬ì¸íŠ¸\r\n@app.route('/generate/order', methods=['GET'])\r\ndef generate_order_logs():\r\n    generate_order_log()\r\n    return jsonify({\"message\": \"Order logs generated!\"})\r\n\r\n@app.route(\"/search/products/years\", methods=[\"GET\"])\r\ndef get_sales_by_year():\r\n    year = request.args.get(\"year\")\r\n    if not year:\r\n        return jsonify({\"error\": \"year parameter required\"}), 400\r\n\r\n    data = get_yearly_sales(year)\r\n    return jsonify(data)\r\n\r\n@app.route(\"/search/products/age\", methods=[\"GET\"])\r\ndef age_group_favorites():\r\n    return jsonify(get_age_group_favorites())\r\n\r\n@app.route(\"/search/products/region\", methods=[\"GET\"])\r\ndef region_favorites():\r\n    return jsonify(get_region_favorites())\r\n\r\n@app.route(\"/search/products/trend\", methods=[\"GET\"])\r\ndef monthly_trend():\r\n    return jsonify(get_monthly_category_trend())\r\n\r\n@app.route(\"/search/products/gender\", methods=[\"GET\"])\r\ndef gender_favorites():\r\n    return jsonify(get_gender_favorites())\r\n\r\n@app.route(\"/search/products/moreSelling\", methods=[\"GET\"])\r\ndef more_selling():\r\n    seller_id = request.args.get(\"sellerId\")\r\n    return jsonify(get_moreSellingProducts(seller_id))\r\n\r\n@app.route(\"/search/products/popularByCategory\", methods=[\"GET\"])\r\ndef popular_by_category():\r\n    seller_id = request.args.get(\"sellerId\")\r\n    return jsonify(get_popularProducts_category(seller_id))\r\n\r\n@app.route(\"/search/products/addedCart\", methods=[\"GET\"])\r\ndef added_cart():\r\n    seller_id = request.args.get(\"sellerId\")\r\n    return jsonify(get_addedCartProducts(seller_id))\r\n\r\n@app.route(\"/search/products/highRated\", methods=[\"GET\"])\r\ndef high_rated():\r\n    seller_id = request.args.get(\"sellerId\")\r\n    return jsonify(get_highRatedProducts(seller_id))\r\n\r\n@app.route(\"/search/products/trending\", methods=[\"GET\"])\r\ndef trending():\r\n    seller_id = request.args.get(\"sellerId\")\r\n    return jsonify(get_trendingProducts(seller_id))\r\n\r\n@app.route(\"/predict/train\", methods=[\"POST\"])\r\ndef train_predict_model():\r\n    algo_name = request.json.get(\"algo_name\")\r\n    print(algo_name)\r\n    result = train_predict_model_and_save(algo_name)\r\n    return jsonify(result)\r\n\r\n@app.route(\"/predict/product\", methods=[\"GET\"])\r\ndef predict_product_quantity():\r\n    product_name = request.args.get(\"productName\")\r\n    algo_name = request.args.get(\"algo\", default=\"linear\")\r\n    print(product_name, \" : \" , algo_name)\r\n    if not product_name:\r\n        return jsonify({\"error\": \"productName parameter is required\"}), 400\r\n\r\n    result = predict_quantity_pipeline(product_name, algo_name)\r\n    return jsonify(result)\r\n\r\n@app.route(\"/recommend/train\", methods=[\"POST\"])\r\ndef train_recommend_model():\r\n    algo_name = request.json.get(\"algo_name\")\r\n    print(algo_name)\r\n    result = train_recommend_model_and_save(algo_name)\r\n    return jsonify(result)\r\n\r\n@app.route(\"/recommend/product\", methods=[\"POST\"])\r\ndef predict_product_recommend():\r\n    user_info = request.json.get(\"user_info\")\r\n    algo_name = request.args.get(\"algo\", default=\"linear\")\r\n\r\n    if not user_info:\r\n        return jsonify({\"error\": \"user_info parameter is required\"}), 400\r\n\r\n    result = predict_recommendation_pipeline(user_info, algo_name)\r\n    return jsonify(result)\r\n\r\nif __name__ == \"__main__\":\r\n    app.run(debug=True)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app.py b/app.py
--- a/app.py	(revision 94921fb68d3aad3ebf0cedff4b49038b84810eb7)
+++ b/app.py	(date 1744360625095)
@@ -120,7 +120,7 @@
 def predict_product_recommend():
     user_info = request.json.get("user_info")
     algo_name = request.args.get("algo", default="linear")
-
+    print(user_info)
     if not user_info:
         return jsonify({"error": "user_info parameter is required"}), 400
 
Index: predict_recommend_product_model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pickle\r\nimport os\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\n\r\nmodel_dir = \"model_storage/recommend\"\r\n\r\ndef encode_user_info(user_info, feature_columns):\r\n    # ì…ë ¥ëœ ìœ ì € ì •ë³´ë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ì¸ì½”ë”©\r\n    user_df = pd.DataFrame([user_info])\r\n\r\n    # ë²”ì£¼í˜• ìˆ˜ì¹˜í™”\r\n    user_encoded = pd.get_dummies(user_df, columns=[\"region\", \"gender\"])\r\n\r\n    # ëˆ„ë½ëœ ë”ë¯¸ ì»¬ëŸ¼ ì¶”ê°€ (product_user_features ê¸°ì¤€ê³¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´)\r\n    for col in feature_columns:\r\n        if col not in user_encoded.columns:\r\n            user_encoded[col] = 0\r\n\r\n    # ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬ (ë™ì¼í•˜ê²Œ ë§ì¶°ì¤˜ì•¼ í•¨)\r\n    user_encoded = user_encoded[feature_columns]\r\n\r\n    return user_encoded\r\n\r\ndef predict_recommendation_pipeline(user_info: dict, algo: str):\r\n    model_path = os.path.join(model_dir, f\"model_{algo}.pkl\")\r\n\r\n    if not os.path.exists(model_path):\r\n        return {\"error\": f\"Model file not found for algorithm: {algo}\"}\r\n\r\n    with open(model_path, \"rb\") as f:\r\n        model_data = pickle.load(f)\r\n\r\n    if algo == \"content\":\r\n        # ì½˜í…ì¸  ê¸°ë°˜: ì‚¬ìš©ìì™€ ìœ ì‚¬í•œ product feature ì°¾ê¸°\r\n        similarity_df = model_data\r\n        # ì‚¬ìš©ì ë²¡í„° ìƒì„±\r\n        user_vector = encode_user_info(user_info, similarity_df.columns).values.reshape(1, -1)\r\n\r\n        # ìœ ì‚¬ë„ ê³„ì‚° (1xN)\r\n        product_vectors = similarity_df.values  # ê° rowëŠ” product vector\r\n        product_names = similarity_df.index\r\n\r\n        cos_scores = cosine_similarity(user_vector, product_vectors).flatten()  # ìœ ì‚¬ë„ ì ìˆ˜ (1ì°¨ì›)\r\n        top_n_idx = cos_scores.argsort()[::-1][:5]  # ë†’ì€ ìˆœì„œ Top 5\r\n\r\n        top_products = product_names[top_n_idx].tolist()\r\n        return {\r\n            \"algorithm\": algo,\r\n            \"recommended_products\": top_products\r\n        }\r\n\r\n    elif algo == \"collaborative\":\r\n        similarity_df = model_data\r\n        # ë‹¨ìˆœíˆ ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ì‚¬ìš©ì 5ëª… â†’ ê·¸ë“¤ì´ ì‚° ì œí’ˆì„ ì¶”ì²œ\r\n        user_id = str(user_info.get(\"userId\", \"\"))\r\n        if user_id not in similarity_df.index:\r\n            return {\"error\": \"User not found in collaborative matrix\"}\r\n\r\n        similar_users = similarity_df.loc[user_id].sort_values(ascending=False).head(5).index.tolist()\r\n        # ê¸°ì¡´ êµ¬ë§¤ ë¡œê·¸ì—ì„œ ë¹„ìŠ·í•œ ì‚¬ìš©ìë“¤ì´ ë§ì´ êµ¬ë§¤í•œ ìƒí’ˆì„ ì¶”ì²œ\r\n        # â€» ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” êµ¬ë§¤ ë¡œê·¸ ì „ì²´ì—ì„œ ìœ ì‚¬ ì‚¬ìš©ì ìƒí’ˆ ìˆ˜ì§‘ í•„ìš”\r\n        return {\r\n            \"algorithm\": algo,\r\n            \"recommended_users\": similar_users,\r\n            \"note\": \"ì¶”ê°€ì ìœ¼ë¡œ ì´ë“¤ì˜ êµ¬ë§¤ ë¡œê·¸ë¥¼ í™œìš©í•´ ì œí’ˆì„ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.\"\r\n        }\r\n\r\n    elif algo == \"svd\":\r\n        svd_model = model_data[\"svd\"]\r\n        user_index = model_data[\"user_index\"]\r\n        item_columns = model_data[\"item_columns\"]\r\n\r\n        # ìƒˆ ì‚¬ìš©ìë¼ë©´ zero ë²¡í„° ë˜ëŠ” í‰ê· ìœ¼ë¡œ ëŒ€ì²´\r\n        user_vector = np.zeros((1, len(item_columns)))\r\n        svd_user_proj = svd_model.transform(user_vector)\r\n        scores = svd_model.inverse_transform(svd_user_proj).flatten()\r\n        top_indices = scores.argsort()[-5:][::-1]\r\n        top_products = [item_columns[i] for i in top_indices]\r\n\r\n        return {\r\n            \"algorithm\": algo,\r\n            \"recommended_products\": top_products\r\n        }\r\n\r\n    elif algo == \"xgb_classifier\":\r\n        model = model_data[\"model\"]\r\n        product_encoder = model_data[\"product_encoder\"]\r\n        region_encoder = model_data[\"region_encoder\"]\r\n\r\n        age = user_info.get(\"age\", 30)\r\n        gender = 0 if user_info.get(\"gender\") == \"M\" else 1\r\n        region = region_encoder.get(user_info.get(\"region\", \"\"), 0)\r\n\r\n        X_input = []\r\n        product_map = {}\r\n        for product, code in product_encoder.items():\r\n            X_input.append([age, gender, region, code])\r\n            product_map[code] = product\r\n\r\n        preds = model.predict_proba(np.array(X_input))[:, 1]\r\n        top_indices = preds.argsort()[-5:][::-1]\r\n        top_products = [product_map[X_input[i][3]] for i in top_indices]\r\n\r\n        return {\r\n            \"algorithm\": algo,\r\n            \"recommended_products\": top_products\r\n        }\r\n    elif algo == \"knn\":\r\n        knn_model = model_data[\"knn\"]\r\n        user_index = model_data[\"user_index\"]\r\n        product_columns = model_data[\"product_columns\"]\r\n        user_matrix = model_data[\"user_item_matrix\"]\r\n\r\n        userId = str(user_info.get(\"userId\", \"\"))\r\n        if userId not in user_index:\r\n            return {\"error\": f\"User '{userId}' not found in training data\"}\r\n\r\n        user_idx = user_index.index(userId)\r\n        user_vector = user_matrix[user_idx].reshape(1, -1)\r\n\r\n        distances, indices = knn_model.kneighbors(user_vector)\r\n        neighbor_indices = indices.flatten()\r\n        neighbor_userIds = [user_index[i] for i in neighbor_indices if i < len(user_index)]\r\n\r\n        from collections import Counter\r\n        neighbor_purchases = []\r\n        for neighbor in neighbor_userIds:\r\n            neighbor_vector = user_matrix[user_index.index(neighbor)]\r\n            purchased = [product_columns[i] for i, val in enumerate(neighbor_vector) if val > 0]\r\n            neighbor_purchases.extend(purchased)\r\n\r\n        top_products = [p for p, _ in Counter(neighbor_purchases).most_common(5)]\r\n        return {\"algorithm\": algo, \"recommended_products\": top_products}\r\n\r\n    else:\r\n        return {\"error\": f\"Unsupported algorithm: {algo}\"}\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/predict_recommend_product_model.py b/predict_recommend_product_model.py
--- a/predict_recommend_product_model.py	(revision 94921fb68d3aad3ebf0cedff4b49038b84810eb7)
+++ b/predict_recommend_product_model.py	(date 1744361412778)
@@ -34,9 +34,10 @@
 
     if algo == "content":
         # ì½˜í…ì¸  ê¸°ë°˜: ì‚¬ìš©ìì™€ ìœ ì‚¬í•œ product feature ì°¾ê¸°
-        similarity_df = model_data
+        similarity_df = model_data["similarity_df"]
+        feature_columns = model_data["feature_columns"]
         # ì‚¬ìš©ì ë²¡í„° ìƒì„±
-        user_vector = encode_user_info(user_info, similarity_df.columns).values.reshape(1, -1)
+        user_vector = encode_user_info(user_info, feature_columns).values.reshape(1, -1)
 
         # ìœ ì‚¬ë„ ê³„ì‚° (1xN)
         product_vectors = similarity_df.values  # ê° rowëŠ” product vector
